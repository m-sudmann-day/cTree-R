
#############################################################################
# cTree.R
#
# Matthew Sudmann-Day
# Barcelona GSE Data Science
#
# Implements a classification tree that supports a variety of cost functions:
# cross-entropy, misclassification error, and Gini index.
#
# Uses R packages:
#   formula.tools
#
# Public functions: cTree(), cTreePredict(), cTreeShow()
#############################################################################

# Activate this code if you do not have the prerequisite libraries.
#install.packages("formula.tools")

#############################################################################
# A private enumeration of cost functions.
costFnc.Entropy = 0
costFnc.ME = 1
costFnc.Gini = 2

#############################################################################
# Public function cTreeShow() prints to the console a friendly version of a
# classification tree that was generated by cTree().
#
# Parameters:
#   tree: the classification tree generated by cTree().
# Returns:
#   nothing
cTreeShow <- function(tree)
{
  cat(paste(rep("  ", tree$level)))
  
  if (tree$nodeType == "leaf")
  {
    cat(paste(c("{ label=", tree$label, " prob=", tree$prob, "} (", tree$points, ")\n"), sep=""))
  }
  else
  {
    cat(paste(tree$col, " <--> ", tree$value, " (", tree$points, ")\n"), sep="")
    cTreeShow(tree$branch1)
    cTreeShow(tree$branch2)
  }
}

#############################################################################
# Public function cTree() builds a classification tree based on the formula,
# data, and hyperparameters passed by the caller.
#
# Parameters:
#   formula: an R-style formula to describe the label and the independent
#     columns in the data
#   data: a data frame containing the training data
#   depth: the maximum depth of split nodes in the tree, excludes leaf nodes
#   minPoints: the minimum number of training observations permitted in a
#     single classification region
#   costFnc: the cost function to measure the fragmentation of the labels
#     (permitted values are "Entropy" (default), "ME", and "Gini")
#
# Calls cTreeAux() to perform the recursive portion of the algorithm.
#
# Returns:
#   a classification tree, consisting of a recursive structure of lists
#   that describe the split and leaf nodes of the tree
cTree <- function(formula, data, depth, minPoints, costFnc="Entropy")
{
  # Use the formula.tools library for parsing the formula.
  require(formula.tools)
  
  # Extract the name of the label column from the formula.
  labelColumn <- get.vars(lhs(formula))
  
  # Extract the names of the feature columns from the formula.
  featureColumns <- get.vars(rhs(formula))
  
  # If the only entry in the feature columns is ".", then use all
  # columns in the data except for the label column.
  if (featureColumns == ".")
  {
    featureColumns <- colnames(data[, colnames(data) != labelColumn])
  }
  
  # Map the cost function string to a cost function number, for cleaner
  # identification within this script.
  costFncNum = -1
  if (costFnc == "Entropy") {
    costFncNum = costFnc.Entropy
  } else if (costFnc == "ME") {
    costFncNum = costFnc.ME
  } else if (costFnc == "Gini") {
    costFncNum = costFnc.Gini
  } else {
    cat("Unrecognized cost function.  Please use Entropy, Gini, or ME.\n")
    return()
  }
  
  # Hand off to cTreeAux to perform the recursion.
  return(cTreeAux(data, labelColumn, featureColumns, depth, minPoints, costFncNum, 1))
}

#############################################################################
# Private function cTreeAux() builds a classification tree based on parameters
# forwarded to it from the call to cTree(), along with a level parameter which
# is expected to be a 1 for the root node.
#
# Returns a list representing the classfication tree.
cTreeAux <- function(data, labelColumn, featureColumns, depth, minPoints, costFncNum, level)
{
  N <- nrow(data)
  
  if (N == 0)
  {
    cat("ERROR")
    return(NULL)
  }
  
  # If the number of points is not too small to split the data set, and the level is not
  # to exceed the maximum depth (counting split nodes only), ...
  if (N >= minPoints * 2 && level <= depth)
  {
    # Call getBestSplit() to find the best split of the data into two separate regions,
    # if any.
    bestSplit <- getBestSplit(data, featureColumns, labelColumn, minPoints, costFncNum)
    
    # If a best split was found that is better than our current single region...
    if (!is.null(bestSplit))
    {
      # Recurse, treating both new regions as new trees, the only difference from a root
      # node being that the level is incremented.
      branch1 <- cTreeAux(bestSplit$set1, labelColumn, featureColumns, depth, minPoints, costFncNum, level+1)
      branch2 <- cTreeAux(bestSplit$set2, labelColumn, featureColumns, depth, minPoints, costFncNum, level+1)
      
      # Determine the depth of the deeper region/branch, and use that as the depth of the
      # deepest desdendant of the current node.
      greatestDepth <- max(branch1$depth, branch2$depth)
      
      # Return a node with a node type (nodeType) of "split", the column to split on (col),
      # the value to split on (value), the depth of the deepest descendant (depth), the level
      # of this node (level), the number of training points in this region (points), and
      # child nodes for the two sides of the split (branch1 and branch2).
      return(list(nodeType="split", col=bestSplit$col, value=bestSplit$value, 
                  depth=greatestDepth, level=level, points=N, branch1=branch1, branch2=branch2))
    }
  }
  
  # If we get here, a split is not going to happen so we build a leaf node containing a
  # node type (nodeType) of "leaf", a winning majority label (label), the probability of
  # that label (prob), the depth of the deepest ancestor of this node which is the node
  # itself (depth), the level of this node (level), and the number of training points in
  # this region (N).
  labels <- data[, labelColumn]
  labelChoice <- getWinningLabel(labels)
  
  return(list(nodeType="leaf", label=labelChoice$label, prob=labelChoice$prob,
              depth=level, level=level, points=N))
}

#############################################################################
# Public function cTreePredict() generates classification predictions based on
# a tree returned from cTree() and test data.
#
# Parameters:
#   tree: a classification tree generated by cTree()
#   data: test data containing feature columns that match those of the training data
#
# Calls cTreePredictAux() to perform the recursive portion of the algorithm.
#
# Returns:
#   a data frame with two columns
#     predLabels: predicted labels
#     prob: the probability of each predicted label
cTreePredict <- function(tree, data)
{
  labels <- c()
  prob <- c()
  
  # Loop through all rows in the test set and call cTreePredictAux() to do the recursive
  # work of determining a prediction.  Store the prediction retrieved and its probability
  # in a pair of vectors
  if (nrow(data) > 0)
  {
    for(row in 1:nrow(data))
    {
      prediction <- cTreePredictAux(tree, data[row, ])
      labels <- c(labels, prediction$predLabel)
      prob <- c(prob, prediction$prob)
    }
  }

  # Convert the labels and probability vectors into a data frame and return.
  return(data.frame(predLabels=labels, prob=prob))
}

#############################################################################
# Private function cTreePredictAux() walks down the nodes of classification tree
# recursively based on a tree that was generated by cTree() (tree) and a row
# of a data frame (row).
#
# Returns a list containing the predicted label (predLabel) and its probability
# (prob).
cTreePredictAux <- function(tree, row)
{
  # If tree is a leaf node, then we simply extract the predicted label that was
  # determined when the tree was built, along with its probability and return.
  if (tree$nodeType == "leaf")
  {
    return(list(predLabel=tree$label, prob=tree$prob))
  }
  # If tree is a split node.
  else
  {
    # Pull the column referenced in the split node and get the corresponding value
    # from the data row.
    dataValue <- row[,tree$col]
    
    # If the value is numeric, apply a comparison between the data value and the
    # value stored in the split node.  Recurse to the first or second child of the
    # tree node accordingly.
    if (is.numeric(tree$value))
    {
      if (dataValue >= tree$value)
      {
        return(cTreePredictAux(tree$branch1, row))
      }
      else
      {
        return(cTreePredictAux(tree$branch2, row))
      }
    }
    # For non-numeric values, the comparison is similar to the above, but instead uses
    # an equality.  Recurse as above.
    else if (dataValue == tree$value)
    {
      return(cTreePredictAux(tree$branch1, row))
    }
    else
    {
      return(cTreePredictAux(tree$branch2, row))
    }
  }
}

#############################################################################
# Private function splitData() splits a data.frame (data) into two sets based
# on the value (value) in a particular column (column).
#
# Returns a list containing the two sets, set1 and set2.
splitData <- function(data, column, value)
{
  # Numeric columns that are not factors will be treated as continuous,
  # and the split involves an inequality.
  if (is.numeric(data[, column])) # int, double
  {
    set1Indexes <- (data[, column] >= value)
  }
  # Non-numeric and factor columns will be treated as discrete entries,
  # and the split involves an equality, meaning that one set will contain
  # one value, and the other set will contain all other values.
  else # character, logical, factor
  {
    set1Indexes <- (data[,column] == value)
  }
  
  # set1Indexes contains logical values indicating whether a row should
  # be placed in set1.  All others go to set 2.
  set1 <- data[set1Indexes, ]
  set2 <- data[!set1Indexes, ]

  return(list(set1=set1, set2=set2))
}

#############################################################################
# Private function getWinningLabel() examines a list of discrete values and
# selects the majority value.  It returns a list containing the majority value
# (label) and its probability (prob).
getWinningLabel <- function(labels)
{
  # The following code, which is probably much more efficient than what
  # is actually used, was not working because it was returning strings
  # for numeric factors.  I cannot reproduce this in isolation, but it
  # is not operating as expected so I chose not to use it.
  #counts <- as.data.frame(table(labels), stringsAsFactors=FALSE)
  #pos <- which.max(counts[,2])
  #label <- counts[pos,1]
  #count <- counts[pos,2]
  
  # Get the unique labels.
  ul <- unique(labels)
  
  # Count them.
  counts <- rep(length(ul), 1)
  for (i in 1:length(ul))
  {
    counts[i] <- sum(labels == ul[i])
  }

  # Find the position of the maximum count.  From that, get the majority
  # label and its probability.
  pos <- which.max(counts)
  label <- ul[pos]
  count <- counts[pos]
  prob <- count/length(labels)
  
  return(list(label=label, prob=prob))
}

#############################################################################
# Private function calculateCost() takes a vector of labels (labels) and
# applies a cost function (costFncNum) to generate a measure of the
# fragmentation of the labels.
calculateCost <- function(labels, costFncNum)
{
  # Get a vector of the counts of each distinct label.
  counts <- as.data.frame(table(labels), stringsAsFactors=FALSE)[,2]
  
  # Normalize around the length of the vector to get a probability for each
  # discrete value.  p becomes a vector of values in [0,1].
  p <- counts/length(labels)
  
  # Based on the cost function requested, apply the appropriate arithmetic.
  if (costFncNum == costFnc.Entropy)
  {
    # Ignore labels that are not in the list because they produce an error
    # and should not affect the entropy anyway.  This only happens because
    # factors contain a list of all possible values, even those not present.
    p <- p[p > 0]
    return (-sum(p * log(p)))
  } else if (costFncNum == costFnc.ME) {
    return(1 - max(p))
  } else if (costFncNum == costFnc.Gini) {
    return(sum(p * (1-p)))
  } else {
    cat("Unrecognized cost function.  Please use Entropy, Gini, or ME")
  }
}

#############################################################################
# Private function getBestSplit() takes a data frame (data), a list of the
# names of the feature columns (featureColumns), the name of the label
# column (labelColumn), the minimum number of points allowed in a region
# (minRowsInSet), and the cost function to apply (costFncNum), and
# finds the best possible split of the data frame that will reduce the
# fragmentation of the labels by the maximum amount possible.
# Returns a list of containing the 
getBestSplit <- function(data, featureColumns, labelColumn, minRowsInSet, costFncNum)
{
  N <- nrow(data)
  
  # Do not perform any processing if there is no possibility of producing a split
  # where both resulting sets are large enough.  This is a redundant check as the
  # same logic is applied in cTreeAux().
  if (N < minRowsInSet * 2)
  {
    return(NULL)
  }
  
  # Calculate the cost/fragmentation of the data in its current form,
  # before the split.
  currentCost <- calculateCost(data[,labelColumn], costFncNum)
  bestSplit <- NULL
  
  # For each unique value in each column...
  for (col in featureColumns)
  {
    uniqueValues <- unique(data[, col])
    for (value in uniqueValues)
    {
      # ...attempt a split.
      sets <- splitData(data, col, value)
      
      n1 <- nrow(sets$set1)
      n2 <- nrow(sets$set2)
      
      # If both halfs are larger than the minimum allowed region size...
      if (n1 >= minRowsInSet && n2 >= minRowsInSet)
      {
        # Calculate the cost/fragmentation of both sets.
        cost1 <- calculateCost(sets$set1[,labelColumn], costFncNum)
        cost2 <- calculateCost(sets$set2[,labelColumn], costFncNum)
        
        p <- n1/N
        
        # Produce a score for the reduction in fragmentation caused by this split.
        totalCost <- currentCost - p*cost1 - (1-p)*cost2
        
        # If the score is not worse than the current situation, and its better
        # than any of the candidates we've tried so far...
        if (totalCost > 0 && (is.null(bestSplit) || totalCost > bestSplit$cost))
        {
          # Build a list to keep track of this current best split.
          bestSplit <- list(cost=totalCost, col=col, value=value,
                            set1=sets$set1, set2=sets$set2)
        }
      }
    }
  }
  
  # Eliminate the cost field because this has no meaning outside the function.
  if (!is.null(bestSplit))
  {
    bestSplit$cost <- NULL
  }

  return(bestSplit)
}

